/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", choices = [True, False], default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", choices = [True, False], default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype == 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + args.C \
                                     + '_solver_' + args.solver + '_penalty_' + args.penalty))
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + args.max_features + '_min_samples_split_' + args.min_samples_split + '_min_samples_leaf_' + \
          args.min_samples_leaf + '_bootstrap_' + args.bootstrap + '_n_estimators_' + args.n_estimators))
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + args.max_features + '_min_samples_split_' + args.min_samples_split + '_min_samples_leaf_' + \
          args.min_samples_leaf + '_splitter_' + args.splitter))
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + args.C + '_class_weight_' + args.class_weight + '_coef0_' + args.coef0 + '_degree_' + args.degree + '_shrinking_' + args.shrinking + '_kernel_' + args.kernel))
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + args.learning_rate + '_dropout_' + args.dropout))

if __name__ == '__main__':
    args = parse_args()
    main(args)

/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", choices = [True, False], default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", choices = [True, False], default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + args.C \
                                     + '_solver_' + args.solver + '_penalty_' + args.penalty))
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + args.max_features + '_min_samples_split_' + args.min_samples_split + '_min_samples_leaf_' + \
          args.min_samples_leaf + '_bootstrap_' + args.bootstrap + '_n_estimators_' + args.n_estimators))
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + args.max_features + '_min_samples_split_' + args.min_samples_split + '_min_samples_leaf_' + \
          args.min_samples_leaf + '_splitter_' + args.splitter))
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + args.C + '_class_weight_' + args.class_weight + '_coef0_' + args.coef0 + '_degree_' + args.degree + '_shrinking_' + args.shrinking + '_kernel_' + args.kernel))
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + args.learning_rate + '_dropout_' + args.dropout))

if __name__ == '__main__':
    args = parse_args()
    main(args)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=500, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False)
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", choices = [True, False], default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", choices = [True, False], default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + args.C \
                                     + '_solver_' + args.solver + '_penalty_' + args.penalty))
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + args.max_features + '_min_samples_split_' + args.min_samples_split + '_min_samples_leaf_' + \
          args.min_samples_leaf + '_bootstrap_' + args.bootstrap + '_n_estimators_' + args.n_estimators))
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + args.max_features + '_min_samples_split_' + args.min_samples_split + '_min_samples_leaf_' + \
          args.min_samples_leaf + '_splitter_' + args.splitter))
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + args.C + '_class_weight_' + args.class_weight + '_coef0_' + args.coef0 + '_degree_' + args.degree + '_shrinking_' + args.shrinking + '_kernel_' + args.kernel))
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + args.learning_rate + '_dropout_' + args.dropout))

if __name__ == '__main__':
    args = parse_args()
    main(args)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=500, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False)
Test Accuracy:0.7988
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", choices = [True, False], default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", choices = [True, False], default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                                     + '_solver_' + args.solver + '_penalty_' + str(args.penalty)))
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)))
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)))
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel))
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)))

if __name__ == '__main__':
    args = parse_args()
    main(args)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=500, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False)
Test Accuracy:0.7988
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", choices = [True, False], default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", choices = [True, False], default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=500, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',
          tol=0.0001, verbose=0, warm_start=False)
Test Accuracy:0.7988
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true', default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true', default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
RandomForestClassifier(bootstrap=True, class_weight='balanced',
            criterion='gini', max_depth=None, max_features='auto',
            max_leaf_nodes=None, min_impurity_decrease=0.0,
            min_impurity_split=None, min_samples_leaf=1,
            min_samples_split=2, min_weight_fraction_leaf=0.0,
            n_estimators=100, n_jobs=None, oob_score=False,
            random_state=None, verbose=0, warm_start=False)
Test Accuracy:0.8040
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true', default = True)
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true', default = True)
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(prob, os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

DecisionTreeClassifier(class_weight='balanced', criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
Test Accuracy:0.6811
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

DecisionTreeClassifier(class_weight='balanced', criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, presort=False, random_state=None,
            splitter='best')
Test Accuracy:0.6733
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = None)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = None)
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

SVC(C=None, cache_size=200, class_weight='balanced', coef0=0,
  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
  kernel='linear', max_iter=-1, probability=True, random_state=None,
  shrinking=False, tol=0.001, verbose=False)
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

SVC(C=1, cache_size=200, class_weight='balanced', coef0=0,
  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
  kernel='linear', max_iter=-1, probability=True, random_state=None,
  shrinking=False, tol=0.001, verbose=False)
Test Accuracy:0.8153
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3b847160>
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3c28da20>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.6384
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        import warnings
        warnings.filterwarnings('ignore')
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3d789b70>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3f39bbe0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.8932
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3e3a8240>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.6028
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a35d82860>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7960
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3a109b38>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.8083
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a42afda90>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7958
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a435d77b8>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.8100
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3a336c18>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7781
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a353fdc18>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7720
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3fd98ba8>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7675
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a4375ec50>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7648
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a35444c50>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7818
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3f411b38>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7969
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3c37d898>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7951
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a352b74a8>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:1.0000
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3ef64be0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:1.0000
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3d4f7358>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.8577
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3809d400>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.6486
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a430a3c18>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.6998
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a42207ba8>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.6963
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a388e3ef0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.7797
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a4275abe0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.8623
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a40b76b70>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
Test Accuracy:0.8428
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a435c9be0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3a433ba8>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
(2621,)
Test Accuracy:0.8573
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a33d63c18>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
(2621, 6)
Test Accuracy:0.8596
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a38c83b38>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a433f3cc0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a4253ab38>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3dce6c18>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
(2621, 6)
Test Accuracy:0.8607
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a34419be0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
{0, 1}
(2621, 6)
Test Accuracy:0.8646
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a36e4bb38>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
{0, 1}
6
0
Test Accuracy:0.8531
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3fe50b38>
0
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
{0, 1}
6
0
Test Accuracy:0.8520
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    print(y_test)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a3e217a58>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = len(set(y_train[1]))
        print(n_class)
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a42103b38>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = classes
        print(n_class)
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a38e61be0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
/Users/cplu/Downloads/Documents/RESEARCH/ml-cleavage/scripts/test.py
# Train and Test
"""
    Usage:
        python test.py -m logistic_regression -i A171T -class 3 --C 1 --solver lbfgs --penalty l2
"""
from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn import svm
import tensorflow as tf
from tensorflow import keras
import warnings
warnings.filterwarnings('ignore')
import pandas as pd
import numpy as np
import os
import argparse
import pickle as pkl
from sklearn import preprocessing
from utils import *

# Argument
def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("-m", "--model", choices = ["logistic_regression", "random_forest", "decision_tree", "svm", "ann"])
    parser.add_argument("-i", "--dataset", type = str) # dataset name, HCV, A171T, D183A, or triple for now
    parser.add_argument("-class", "--classes", type = int) # number of classes, 2 or 3
    parser.add_argument("-C","--C", type = float, default = 1)
    parser.add_argument("-solver", "--solver", choices = ["newton-cg","lbfgs","liblinear","sag","saga"], default = 'lbfgs')
    parser.add_argument("-penalty", "--penalty", choices = ["l1","l2"], default = "l2")
    parser.add_argument("-cw", "--class_weight", choices = ["balanced", "balanced_subsample", None], default="balanced")
    parser.add_argument("-criter", "--criterion", choices = ["gini", "entropy"], default = "gini")
    parser.add_argument("-mf", "--max_features", choices = ["auto", "log2", None], default = "auto")
    parser.add_argument("-mss","--min_samples_split", type=int, default = 2)
    parser.add_argument("-msl", "--min_samples_leaf", type=int, default = 1)
    parser.add_argument("-bs", "--bootstrap", action = 'store_true')
    parser.add_argument("-ne", "--n_estimators", type=int, default=100)
    parser.add_argument("-split", "--splitter", choices = ["best", "random"], default = "best")
    parser.add_argument("-coef", "--coef0", type = float, default=0)
    parser.add_argument("-degree", "--degree", type = int, default = 3)
    parser.add_argument("-shrinking", "--shrinking", action = 'store_true')
    parser.add_argument("-kernel", "--kernel", choices = ["linear", "poly", "rbf", "sigmoid"], default="linear")
    parser.add_argument("-energy", "--energy_only", action = "store_true")
    parser.add_argument("-lr", "--learning_rate", type=float, default = 0.001)
    parser.add_argument("-dropout","--dropout", type=float, default = 0)
    parser.add_argument("-save", "--save", type = str, default = "./") # path of saving logits
    args = parser.parse_args()
    return args

def main(args):
    dataset = args.dataset
    classes = args.classes
    model = args.model
    X_train, y_train, X_test, y_test = load_data(dataset, classes)
    if args.energy_only == True:
        X_train = X_train.iloc[:,100:].copy()
        X_test = X_test.iloc[:, 100:].copy()
    X_train = scale(X_train)
    X_test = scale(X_test)
    logger = get_logger(logpath=os.path.join(args.save, 'logs'),filepath=os.path.abspath(__file__))
    if classes == 2:
        classtype = 'binary'
    elif classes == 3:
        classtype = 'ternary'

    if model == 'logistic_regression':
        if args.C != None:
            C = args.C
        else:
            C = 1
        if args.penalty != None:
            penalty = args.penalty
        else:
            penalty = 'l2' 
        from sklearn import linear_model
        lg = linear_model.LogisticRegression(C = args.C, solver = args.solver, penalty = args.penalty, max_iter = 500)
        logger.info(lg)
        prob, acc = train_test(lg, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))

        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) \
                           + '_solver_' + args.solver + '_penalty_' + str(args.penalty)), prob)
    elif model == 'random_forest':
        av_acc = 0
        for i in range(20):
            from sklearn.ensemble import RandomForestClassifier
            rf = RandomForestClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, bootstrap = args.bootstrap, n_estimators=args.n_estimators)
            logger.info(rf)
            prob, acc = train_test(rf, X_train, y_train, X_test, y_test)
            av_acc += acc
        av_acc = av_acc / 20
        logger.info('Test Accuracy:{:.4f}'.format(av_acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_bootstrap_' + str(args.bootstrap) + '_n_estimators_' + str(args.n_estimators)), prob)
    elif model == 'decision_tree':
        from sklearn.tree import DecisionTreeClassifier
        dt = DecisionTreeClassifier(class_weight = args.class_weight, criterion = args.criterion, max_features = args.max_features, min_samples_split = args.min_samples_split, min_samples_leaf = args.min_samples_leaf, splitter = args.splitter)
        logger.info(dt)
        prob, acc = train_test(dt, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_class_weight_' + args.class_weight + '_criterion_' + args.criterion + '_max_features_' + str(args.max_features) + '_min_samples_split_' + str(args.min_samples_split) + '_min_samples_leaf_' + \
          str(args.min_samples_leaf) + '_splitter_' + str(args.splitter)), prob)
    elif model == 'svm':
        from sklearn import svm
        svmsvc = svm.SVC(C = args.C, class_weight = args.class_weight, coef0 = args.coef0, degree = args.degree, shrinking = args.shrinking, kernel = args.kernel, probability=True)
        logger.info(svmsvc)
        prob, acc = train_test(svmsvc, X_train, y_train, X_test, y_test)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_C_' + str(args.C) + '_class_weight_' + args.class_weight + '_coef0_' + str(args.coef0) + '_degree_' + str(args.degree) + '_shrinking_' + str(args.shrinking) + '_kernel_' + args.kernel), prob)
    elif model == 'ann':
        import tensorflow as tf
        from tensorflow import keras
        os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'
        dropout = args.dropout
        lr = args.learning_rate
        n_class = classes
        ann = keras.Sequential([keras.layers.Dense(1024, activation=tf.nn.relu),
                                  keras.layers.Dropout(dropout, input_shape = (1024,)),
                                  keras.layers.Dense(n_class, activation=tf.nn.softmax)])

        ann.compile(optimizer=tf.train.AdamOptimizer(learning_rate = lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])
        logger.info(ann)
        prob, acc = train_test_ann(ann, n_class, X_train, y_train, X_test, y_test,logger)
        logger.info('Test Accuracy:{:.4f}'.format(acc))
        np.savetxt(os.path.join(args.save, 'logits_' + args.model + '_' + str(dataset) + '_' + classtype + '_lr_' + str(args.learning_rate) + '_dropout_' + str(args.dropout)), prob)

if __name__ == '__main__':
    args = parse_args()
    main(args)

From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.
Instructions for updating:
Colocations handled automatically by placer.
<tensorflow.python.keras.engine.sequential.Sequential object at 0x1a35a8fbe0>
From /Users/cplu/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/layers/core.py:143: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.
Instructions for updating:
Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.
{0, 1, 2}
414
Test Accuracy:0.8337
